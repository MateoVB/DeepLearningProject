{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-Based Spam Filtering Project by Mateo Velarde & Mohammad Fanous\n",
    "\n",
    "In this project, we explore the application of the state-of-the-art natural language processing model, Bidirectional Encoder Representations from Transformers (BERT), for the purpose of spam detection.\n",
    "\n",
    "## Milestone 1: \n",
    "Download and preprocess the data that will be used in your project work. The result of the code should be input (X) and output (Y) matrices, which can be later used for training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3343, 238)\n",
      "X_val shape: (1114, 238)\n",
      "X_test shape: (1115, 238)\n",
      "Y_train shape: (3343,)\n",
      "Y_val shape: (1114,)\n",
      "Y_test shape: (1115,)\n",
      "[[  101  2175  2127 ...     0     0     0]\n",
      " [  101  7929  2474 ...     0     0     0]\n",
      " [  101  2489  4443 ...     0     0     0]\n",
      " ...\n",
      " [  101 12063  1010 ...     0     0     0]\n",
      " [  101  1996  3124 ...     0     0     0]\n",
      " [  101 20996 10258 ...     0     0     0]] [0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the dataset is downloaded and available as 'spam.csv'\n",
    "# Read the dataset\n",
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "data = data[['v1', 'v2']] # v1: label, v2: message\n",
    "data.columns = ['label', 'message']\n",
    "\n",
    "# Preprocessing\n",
    "# Remove stop words or other preprocessing steps if needed\n",
    "# For simplicity, this example doesn't include extensive preprocessing\n",
    "\n",
    "# Label encoding (spam: 1, not spam: 0)\n",
    "data['label'] = data['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the messages and create the input matrix (X)\n",
    "tokenized = data['message'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Pad the tokenized data to have the same length\n",
    "max_len = max([len(i) for i in tokenized])\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized])\n",
    "\n",
    "# Prepare input (X) and output (Y) matrices\n",
    "X = np.array(padded)\n",
    "Y = data['label'].values\n",
    "\n",
    "# Split the dataset into training (60%), validation (20%), and test sets (20%)\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# The X_train, X_val, X_test, Y_train, Y_val, Y_test are now ready to be used for training and evaluation\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_val shape:', Y_val.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n",
    "\n",
    "print(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
